{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing all files...\n",
      "Fitting TF-IDF vectorizer on complete dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
   
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling complete dataset...\n",
      "Applying PCA on complete dataset...\n",
      "\n",
      "Complete dataset statistics:\n",
      "Original dimensions: 5000\n",
      "Reduced dimensions: 521\n",
      "Explained variance ratio sum: 0.5004\n",
      "\n",
      "Top features in first 3 principal components:\n",
      "\n",
      "PC1 top features: முடியாதவர்கள், தேர்தலில் அவர்களுக்கு, கொண்டு வந்தோம், காட்டி செல்கிறார்கள், காட்டுவோம் செந்தமிழன், show நடத்தி, டாட்டா காட்டுவோம், நாமும் தேர்தலில், டாட்டா காட்டி, கேட்க முடியாதவர்கள்\n",
      "\n",
      "PC2 top features: alert இன்று, செல்வோம் ்களின், தொடர்ச்சியாகப், முதல் வரைக்கும், trend alert, பதிவுகள் செய்து, கீழ்கண்ட, முழுவதும் கொண்டு, வரைக்கும் கீழ்கண்ட, கீழ்கண்ட கொத்துக்குறியை\n",
      "\n",
      "PC3 top features: மேல் அப்பா, திருடன் கருணாநிதி, விட அப்பா, திருடனே, திருடனை விட, திருடன் ஸ்டாலின், கொண்டிருக்கிறார் செந்தமிழன், மகன் திருடனை, திருடனே மேல், அப்பா திருடனே\n",
      "Saved reduced vectors to data/PS_train_tfidf_reduced.csv\n",
      "Saved reduced vectors to data/PS_dev_tfidf_reduced.csv\n",
      "Saved reduced vectors to data/PS_test_tfidf_reduced.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "def preprocess_tamil_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase (for any English text mixed in)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Remove English punctuation\n",
    "    text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "    \n",
    "    # Remove Tamil punctuation marks\n",
    "    tamil_punctuation = '।॥॰'\n",
    "    text = re.sub(f'[{tamil_punctuation}]', ' ', text)\n",
    "    \n",
    "    # Remove numbers (both English and Tamil)\n",
    "    text = re.sub(r'[\\u0BE6-\\u0BEF\\u0030-\\u0039]', '', text)\n",
    "    \n",
    "    # Remove Zero-width non-joiner and Zero-width joiner\n",
    "    text = text.replace('\\u200c', '').replace('\\u200d', '')\n",
    "    \n",
    "    # Normalize Unicode characters\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Remove emojis and special characters\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Remove repeated Tamil characters (more than 2 repetitions)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    file_paths = ['data/PS_train.csv', 'data/PS_dev.csv', 'data/PS_test.csv']\n",
    "    \n",
    "    print(\"Loading and preprocessing all files...\")\n",
    "    all_dataframes = []\n",
    "    original_lengths = []\n",
    "    \n",
    "    # Load and preprocess all files\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['processed_content'] = df['content'].fillna('').apply(preprocess_tamil_text)\n",
    "        original_lengths.append(len(df))\n",
    "        all_dataframes.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Create and fit TF-IDF vectorizer on all data\n",
    "    print(\"Fitting TF-IDF vectorizer on complete dataset...\")\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        tokenizer=lambda x: x.split(),\n",
    "        ngram_range=(1, 2),\n",
    "        strip_accents=None\n",
    "    )\n",
    "    \n",
    "    # Transform all data at once\n",
    "    tfidf_vectors = tfidf.fit_transform(combined_df['processed_content'])\n",
    "    tfidf_dense = tfidf_vectors.toarray()\n",
    "    \n",
    "    # Scale all data together\n",
    "    print(\"Scaling complete dataset...\")\n",
    "    scaler = StandardScaler()\n",
    "    tfidf_scaled = scaler.fit_transform(tfidf_dense)\n",
    "    \n",
    "    # Apply PCA on all data together\n",
    "    print(\"Applying PCA on complete dataset...\")\n",
    "    pca = PCA(n_components=0.50)\n",
    "    all_reduced_vectors = pca.fit_transform(tfidf_scaled)\n",
    "    \n",
    "    # Print overall statistics\n",
    "    print(f\"\\nComplete dataset statistics:\")\n",
    "    print(f\"Original dimensions: {tfidf_dense.shape[1]}\")\n",
    "    print(f\"Reduced dimensions: {all_reduced_vectors.shape[1]}\")\n",
    "    print(f\"Explained variance ratio sum: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "    \n",
    "    # Save PCA components\n",
    "    pca_components_df = pd.DataFrame(\n",
    "        pca.components_,\n",
    "        columns=tfidf.get_feature_names_out()\n",
    "    )\n",
    "    pca_components_df.to_csv('global_pca_components.csv')\n",
    "    \n",
    "    # Print top features\n",
    "    print(\"\\nTop features in first 3 principal components:\")\n",
    "    for i in range(min(3, len(pca.components_))):\n",
    "        top_features_idx = np.abs(pca.components_[i]).argsort()[-10:][::-1]\n",
    "        top_features = [tfidf.get_feature_names_out()[idx] for idx in top_features_idx]\n",
    "        print(f\"\\nPC{i+1} top features:\", \", \".join(top_features))\n",
    "    \n",
    "    # Split back into original files and save\n",
    "    start_idx = 0\n",
    "    for file_path, length in zip(file_paths, original_lengths):\n",
    "        end_idx = start_idx + length\n",
    "        \n",
    "        # Get the original dataframe\n",
    "        df = all_dataframes[file_paths.index(file_path)]\n",
    "        \n",
    "        # Add reduced vectors for this file\n",
    "        df['tfidf_reduced'] = [vector.tolist() for vector in all_reduced_vectors[start_idx:end_idx]]\n",
    "        \n",
    "        # Create output filename\n",
    "        output_path = file_path.replace('.csv', '_tfidf_reduced.csv')\n",
    "        \n",
    "        # Save to new CSV file\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved reduced vectors to {output_path}\")\n",
    "        \n",
    "        start_idx = end_idx\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
